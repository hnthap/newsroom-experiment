{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment on Vietnamese word segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "garbage collector collected 0 objects\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "import os\n",
    "import time\n",
    "\n",
    "\n",
    "def collect(*, verbose=True):\n",
    "    if verbose:\n",
    "        print('garbage collector collected %d objects' % gc.collect())\n",
    "    else:\n",
    "        gc.collect()\n",
    "\n",
    "collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Markdown programmatically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "*hello*, **world**!"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Markdown, display\n",
    "\n",
    "\n",
    "display(Markdown('*hello*, **world**!'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vietnamese Word Segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: \"cuda\"\n",
      "Device name: \"NVIDIA GeForce RTX 3050 Laptop GPU\"\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print('Device: \"%s\"' % device)\n",
    "if torch.cuda.is_available():\n",
    "    print('Device name: \"%s\"' % torch.cuda.get_device_name(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda\n"
     ]
    }
   ],
   "source": [
    "from transformers import (\n",
    "    AutoModelForTokenClassification,\n",
    "    AutoTokenizer,\n",
    "    pipeline\n",
    ")\n",
    "\n",
    "\n",
    "ws_tokenizer = AutoTokenizer.from_pretrained(\n",
    "    'NlpHUST/vi-word-segmentation',\n",
    "    truncate=True,\n",
    "    model_max_length=512,\n",
    ")\n",
    "ws_model = AutoModelForTokenClassification.from_pretrained(\n",
    "    'NlpHUST/vi-word-segmentation',\n",
    "    torch_dtype=torch.float16,\n",
    "    max_length=512,\n",
    ")\n",
    "ws = pipeline(\n",
    "    'token-classification',\n",
    "    model=ws_model,\n",
    "    tokenizer=ws_tokenizer,\n",
    "    device=device,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_vietnamese(\n",
    "    texts: list[str],\n",
    "    *,\n",
    "    batch_size = 128,\n",
    ") -> list[str]:\n",
    "    if len(texts) == 0:\n",
    "        return []\n",
    "    global ws\n",
    "    results = ws(texts, batch_size=batch_size)\n",
    "    segments = []\n",
    "    for doc in results:\n",
    "        tokens = ''\n",
    "        for e in doc:\n",
    "            word = e['word']\n",
    "            if '##' in word:\n",
    "                tokens += word.replace('##', '')\n",
    "            elif e['entity'] == 'I':\n",
    "                tokens += '_' + word\n",
    "            else:\n",
    "                tokens += ' ' + word\n",
    "        segments.append(tokens.lstrip())\n",
    "    return segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "delta (seconds): 0.0360\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "\n",
       "Không ai có thể bị bắt làm nô lệ hoặc nô dịch; chế độ nô lệ và việc buôn nô lệ\n",
       "bị cấm dưới mọi hình thức.\n",
       "\n",
       "\n",
       "`Không ai có_thể bị bắt làm nô_lệ hoặc nô_dịch ; chế_độ nô_lệ và việc buôn nô_lệ bị cấm dưới mọi hình_thức .`\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "example = \\\n",
    "'''\n",
    "Không ai có thể bị bắt làm nô lệ hoặc nô dịch; chế độ nô lệ và việc buôn nô lệ\n",
    "bị cấm dưới mọi hình thức.\n",
    "''' \n",
    "now = time.time()\n",
    "tokenized_example = tokenize_vietnamese([example])[0]\n",
    "delta = time.time() - now\n",
    "print('delta (seconds): %.4f' % delta)\n",
    "\n",
    "display(Markdown(\n",
    "f'''\n",
    "{example}\n",
    "\n",
    "`{tokenized_example}`\n",
    "'''\n",
    "))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "newsroom",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
